"""
Pipeline Manager - Qu·∫£n l√Ω v√† ƒëi·ªÅu ph·ªëi to√†n b·ªô pipeline
"""
import logging
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import asyncio
import concurrent.futures
from dataclasses import dataclass

from .image_generation import ModelGenerator
from .trend_analysis import TrendAnalyzer
from .content_analysis import ContentAnalyzer
from .script_generation import ScriptGenerator
from .video_production import VideoProducer
from .video_editing import VideoEditor

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PipelineConfig:
    """C·∫•u h√¨nh pipeline"""
    keywords: List[str]
    target_product: str
    video_duration: int
    output_dir: Path
    max_parallel_tasks: int = 3
    enable_gpu: bool = True
    quality_preset: str = "high"  # low, medium, high, ultra

class PipelineManager:
    """Qu·∫£n l√Ω v√† ƒëi·ªÅu ph·ªëi pipeline AI Video Marketing"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.components = {}
        self._initialize_components()
        
    def _initialize_components(self):
        """Kh·ªüi t·∫°o c√°c component"""
        try:
            logger.info("Initializing pipeline components...")
            
            self.components = {
                'model_generator': ModelGenerator(self.config['ai']['stable_diffusion']),
                'trend_analyzer': TrendAnalyzer(self.config['trend']),
                'content_analyzer': ContentAnalyzer(self.config['ai']['openai']),
                'script_generator': ScriptGenerator(self.config['ai']['openai']),
                'video_producer': VideoProducer(self.config['video']),
                'video_editor': VideoEditor(self.config['video'])
            }
            
            logger.info("‚úì All components initialized successfully")
            
        except Exception as e:
            logger.error(f"Error initializing components: {e}")
            raise
    
    async def run_async_pipeline(self, pipeline_config: PipelineConfig) -> Dict[str, Any]:
        """
        Ch·∫°y pipeline b·∫•t ƒë·ªìng b·ªô ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t
        
        Args:
            pipeline_config: C·∫•u h√¨nh pipeline
            
        Returns:
            Dict[str, Any]: K·∫øt qu·∫£ pipeline
        """
        logger.info("üöÄ Starting async pipeline...")
        
        pipeline_results = {
            "pipeline_info": {
                "started_at": datetime.now().isoformat(),
                "config": pipeline_config.__dict__,
                "mode": "async"
            },
            "steps": {}
        }
        
        try:
            # Step 1: Trend Analysis (c√≥ th·ªÉ ch·∫°y song song v·ªõi model generation)
            logger.info("üìä Step 1: Starting trend analysis...")
            trend_task = asyncio.create_task(
                self._run_trend_analysis_async(pipeline_config)
            )
            
            # Step 2: Model Generation (ch·∫°y song song)
            logger.info("üë§ Step 2: Starting model generation...")
            model_task = asyncio.create_task(
                self._run_model_generation_async(pipeline_config)
            )
            
            # Ch·ªù c·∫£ hai task ho√†n th√†nh
            trend_result, model_result = await asyncio.gather(trend_task, model_task)
            
            pipeline_results["steps"]["trend_analysis"] = trend_result
            pipeline_results["steps"]["model_generation"] = model_result
            
            # Step 3: Content Analysis (ph·ª• thu·ªôc v√†o trend analysis)
            logger.info("üîç Step 3: Starting content analysis...")
            content_result = await self._run_content_analysis_async(
                pipeline_config, trend_result
            )
            pipeline_results["steps"]["content_analysis"] = content_result
            
            # Step 4: Script Generation (ph·ª• thu·ªôc v√†o trend v√† content analysis)
            logger.info("üìù Step 4: Starting script generation...")
            script_result = await self._run_script_generation_async(
                pipeline_config, trend_result, content_result
            )
            pipeline_results["steps"]["script_generation"] = script_result
            
            # Step 5: Video Production (ph·ª• thu·ªôc v√†o script v√† model images)
            logger.info("üé¨ Step 5: Starting video production...")
            video_result = await self._run_video_production_async(
                pipeline_config, script_result, model_result
            )
            pipeline_results["steps"]["video_production"] = video_result
            
            # Step 6: Video Editing (ph·ª• thu·ªôc v√†o video production)
            logger.info("‚úÇÔ∏è Step 6: Starting video editing...")
            editing_result = await self._run_video_editing_async(
                pipeline_config, script_result, video_result
            )
            pipeline_results["steps"]["video_editing"] = editing_result
            
            # Pipeline completed
            pipeline_results["pipeline_info"]["completed_at"] = datetime.now().isoformat()
            pipeline_results["pipeline_info"]["status"] = "completed"
            pipeline_results["pipeline_info"]["success"] = True
            
            logger.info("üéâ Async pipeline completed successfully!")
            return pipeline_results
            
        except Exception as e:
            logger.error(f"‚ùå Async pipeline failed: {e}")
            pipeline_results["pipeline_info"]["status"] = "failed"
            pipeline_results["pipeline_info"]["error"] = str(e)
            pipeline_results["pipeline_info"]["completed_at"] = datetime.now().isoformat()
            raise
    
    async def _run_trend_analysis_async(self, config: PipelineConfig) -> Dict[str, Any]:
        """Ch·∫°y trend analysis b·∫•t ƒë·ªìng b·ªô"""
        def run_trend_analysis():
            analyzer = self.components['trend_analyzer']
            videos = analyzer.search_trending_keywords(config.keywords, max_results=50)
            analysis = analyzer.analyze_trending_patterns(videos)
            
            # Save results
            output_dir = config.output_dir / "trend_analysis"
            output_dir.mkdir(parents=True, exist_ok=True)
            analyzer.save_analysis(videos, analysis, output_dir)
            
            return {
                "status": "completed",
                "videos_found": len(videos),
                "output_dir": str(output_dir)
            }
        
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(executor, run_trend_analysis)
        
        return result
    
    async def _run_model_generation_async(self, config: PipelineConfig) -> Dict[str, Any]:
        """Ch·∫°y model generation b·∫•t ƒë·ªìng b·ªô"""
        def run_model_generation():
            generator = self.components['model_generator']
            
            model_config = {
                "gender": "female",
                "age": "young adult",
                "ethnicity": "asian",
                "style": "professional",
                "setting": "studio",
                "clothing": "business casual"
            }
            
            output_dir = config.output_dir / "model_images"
            portfolio = generator.create_model_portfolio(model_config, output_dir)
            
            # Count generated images
            total_images = sum(len(paths) for paths in portfolio["images"].values())
            
            return {
                "status": "completed",
                "images_generated": total_images,
                "output_dir": str(output_dir)
            }
        
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(executor, run_model_generation)
        
        return result
    
    async def _run_content_analysis_async(self, 
                                        config: PipelineConfig,
                                        trend_result: Dict[str, Any]) -> Dict[str, Any]:
        """Ch·∫°y content analysis b·∫•t ƒë·ªìng b·ªô"""
        def run_content_analysis():
            analyzer = self.components['content_analyzer']
            
            # Load trend analysis results
            trend_file = Path(trend_result["output_dir"]) / "trend_analysis_*.json"
            # Implementation would load the actual trend data here
            
            # For now, use mock data
            mock_videos = [{"title": f"Video {i}", "description": f"Description {i}"} 
                          for i in range(10)]
            
            output_dir = config.output_dir / "content_analysis"
            results = analyzer.batch_analyze_videos(mock_videos, output_dir)
            
            return {
                "status": "completed",
                "videos_analyzed": len(results),
                "output_dir": str(output_dir)
            }
        
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(executor, run_content_analysis)
        
        return result
    
    async def _run_script_generation_async(self,
                                         config: PipelineConfig,
                                         trend_result: Dict[str, Any],
                                         content_result: Dict[str, Any]) -> Dict[str, Any]:
        """Ch·∫°y script generation b·∫•t ƒë·ªìng b·ªô"""
        def run_script_generation():
            generator = self.components['script_generator']
            
            # Mock trend analysis data
            trend_analysis = {"videos": [], "analysis": {}}
            content_analysis = []
            
            script = generator.generate_video_script(
                trend_analysis=trend_analysis,
                content_analysis=content_analysis,
                target_product=config.target_product,
                video_duration=config.video_duration
            )
            
            # Save script
            output_dir = config.output_dir / "scripts"
            output_dir.mkdir(parents=True, exist_ok=True)
            script_file = generator.save_script(script, output_dir)
            
            return {
                "status": "completed",
                "output_file": str(script_file),
                "success_probability": script.get("insights", {}).get("success_probability", 0)
            }
        
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(executor, run_script_generation)
        
        return result
    
    async def _run_video_production_async(self,
                                        config: PipelineConfig,
                                        script_result: Dict[str, Any],
                                        model_result: Dict[str, Any]) -> Dict[str, Any]:
        """Ch·∫°y video production b·∫•t ƒë·ªìng b·ªô"""
        def run_video_production():
            producer = self.components['video_producer']
            
            # Load script
            script_file = Path(script_result["output_file"])
            with open(script_file, 'r', encoding='utf-8') as f:
                script = json.load(f)
            
            # Load model images
            model_images = []
            model_dir = Path(model_result["output_dir"])
            for img_file in model_dir.rglob("*.png"):
                from PIL import Image
                model_images.append(Image.open(img_file))
            
            # Create video
            output_path = config.output_dir / "main_video.mp4"
            video_info = producer.create_video_from_script(script, model_images, output_path)
            
            return {
                "status": "completed",
                "output_file": str(output_path),
                "duration": video_info["duration"]
            }
        
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(executor, run_video_production)
        
        return result
    
    async def _run_video_editing_async(self,
                                     config: PipelineConfig,
                                     script_result: Dict[str, Any],
                                     video_result: Dict[str, Any]) -> Dict[str, Any]:
        """Ch·∫°y video editing b·∫•t ƒë·ªìng b·ªô"""
        def run_video_editing():
            editor = self.components['video_editor']
            
            # Load script
            script_file = Path(script_result["output_file"])
            with open(script_file, 'r', encoding='utf-8') as f:
                script = json.load(f)
            
            # Edit video
            video_path = video_result["output_file"]
            final_video_info = editor.create_final_video(
                [video_path], script, config.output_dir / "final_video.mp4"
            )
            
            # Create short clips
            short_clips = editor.create_short_clips(
                video_path, script, config.output_dir / "short_clips"
            )
            
            return {
                "status": "completed",
                "output_files": final_video_info["output_paths"],
                "short_clips_created": len(short_clips)
            }
        
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(executor, run_video_editing)
        
        return result
    
    def run_batch_pipeline(self, 
                          pipeline_configs: List[PipelineConfig]) -> List[Dict[str, Any]]:
        """
        Ch·∫°y nhi·ªÅu pipeline song song
        
        Args:
            pipeline_configs: Danh s√°ch c·∫•u h√¨nh pipeline
            
        Returns:
            List[Dict[str, Any]]: K·∫øt qu·∫£ c·ªßa t·∫•t c·∫£ pipeline
        """
        logger.info(f"üöÄ Starting batch pipeline with {len(pipeline_configs)} configurations...")
        
        async def run_all_pipelines():
            tasks = []
            for config in pipeline_configs:
                task = asyncio.create_task(self.run_async_pipeline(config))
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
        
        # Run all pipelines
        results = asyncio.run(run_all_pipelines())
        
        # Process results
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Pipeline {i+1} failed: {result}")
                processed_results.append({
                    "pipeline_index": i,
                    "status": "failed",
                    "error": str(result)
                })
            else:
                result["pipeline_index"] = i
                processed_results.append(result)
        
        logger.info(f"üéâ Batch pipeline completed: {len(processed_results)} results")
        return processed_results
    
    def get_pipeline_status(self, output_dir: Path) -> Dict[str, Any]:
        """
        L·∫•y tr·∫°ng th√°i pipeline t·ª´ th∆∞ m·ª•c output
        
        Args:
            output_dir: Th∆∞ m·ª•c output
            
        Returns:
            Dict[str, Any]: Tr·∫°ng th√°i pipeline
        """
        status = {
            "output_dir": str(output_dir),
            "exists": output_dir.exists(),
            "steps_completed": [],
            "files_generated": []
        }
        
        if not output_dir.exists():
            return status
        
        # Check for completed steps
        step_dirs = [
            "trend_analysis",
            "content_analysis", 
            "model_images",
            "scripts",
            "short_clips"
        ]
        
        for step_dir in step_dirs:
            if (output_dir / step_dir).exists():
                status["steps_completed"].append(step_dir)
        
        # Check for generated files
        for file_path in output_dir.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.mp4', '.json', '.png', '.jpg']:
                status["files_generated"].append(str(file_path.relative_to(output_dir)))
        
        return status

# Example usage
if __name__ == "__main__":
    from configs.config import AI_CONFIG, TREND_CONFIG, VIDEO_CONFIG
    
    # Initialize pipeline manager
    config = {
        'ai': AI_CONFIG,
        'trend': TREND_CONFIG,
        'video': VIDEO_CONFIG
    }
    
    manager = PipelineManager(config)
    
    # Create pipeline config
    pipeline_config = PipelineConfig(
        keywords=["cooking", "tips", "viral"],
        target_product="Cooking Course",
        video_duration=60,
        output_dir=Path("outputs/test_pipeline")
    )
    
    # Run async pipeline
    async def main():
        result = await manager.run_async_pipeline(pipeline_config)
        print("Pipeline completed:", result)
    
    asyncio.run(main())
